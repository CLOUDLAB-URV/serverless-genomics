import logging
import os
import re
import multiprocessing
import shutil
import subprocess
import subprocess as sp
import tempfile
import uuid
from typing import Tuple
import pandas as pd
from numpy import int64

from .data_fetch import fetch_fastq_chunk, fetch_fasta_chunk
# from ..preprocessing.preprocess_fastq import fastq_to_mapfun
from .. import aux_functions as aux
from ..utils import copy_to_runtime, force_delete_local_path
from ..parameters import PipelineRun
from lithops import Storage
import zipfile

logger = logging.getLogger(__name__)


def gem_indexer_mapper(pipeline_params: PipelineRun, mapper_id: str, fasta_chunk: dict, fastq_chunk: dict,
                       storage: Storage):
    """
    First map function to filter and map fasta + fastq chunks. Some intermediate files
    are uploaded into the cloud storage for subsequent index correction, after which
    the final part of the map function (map_alignment2) can be executed.

    Args:
        mapper_id (int): mapper id
        fasta_chunk (dict): contains all the necessary info related to the fasta chunk
        fastq_chunk (str): fastq chunk key
        exec_param (str): string used to differentiate this pipeline execution from others with different parameters
        storage (Storage): s3 storage instance, generated by lithops

    Returns:
        Tuple[str]: multiple values needed for index correction and the second map phase
    """
    tmp_dir = tempfile.mkdtemp()
    pwd = os.getcwd()
    os.chdir(tmp_dir)

    try:
        # Get fastq chunk and store it to disk in tmp directory
        fastq_chunk_filename = f"chunk_{fastq_chunk['chunk_id']}.fastq"
        fastqgz_idx_key, _ = pipeline_params.fastqgz_idx_keys
        fetch_fastq_chunk(fastq_chunk, fastq_chunk_filename, storage, pipeline_params.fastq_path,
                          pipeline_params.storage_bucket, fastqgz_idx_key)

        # Get fasta chunk and store it to disk in tmp directory
        fasta_chunk_filename = f"chunk_{fasta_chunk['chunk_id']}.fasta"
        fetch_fasta_chunk(fasta_chunk, fasta_chunk_filename, storage, pipeline_params.fasta_path)

        gem_index_filename = os.path.join(f'{mapper_id}.gem')
        cpus = multiprocessing.cpu_count()
        # gem-indexer appends .gem to output file
        cmd = ['gem-indexer', '--input', fasta_chunk_filename, '--threads', str(cpus), '-o',
               gem_index_filename.replace('.gem', '')]
        print(' '.join(cmd))
        out = sp.run(cmd, capture_output=True)
        print(out.stderr.decode('utf-8'))
        # TODO apparently 1 is return code for success (why)
        assert out.returncode == 1

        # GENERATE ALIGNMENT AND ALIGNMENT INDEX (FASTQ TO MAP)
        # TODO refactor bash script
        # TODO support implement paired-end, replace not-used with 2nd fastq chunk
        # TODO use proper tmp directory instead of uuid base name
        # TODO add support for sra source
        base_name = 'SRRXXXXXX'
        cmd = ['/function/bin/map_index_and_filter_map_file_cmd_awsruntime.sh', gem_index_filename,
               fastq_chunk_filename, "not-used", base_name, "s3", "single-end"]
        print(' '.join(cmd))
        out = sp.run(cmd, capture_output=True)
        print(out.stdout.decode('utf-8'))
        print(out.stderr.decode('utf-8'))

        print(os.listdir('.'))

        # Reorganize file names
        map_index_filename = os.path.join(tmp_dir, base_name + "_map.index.txt")
        shutil.move(base_name + "_map.index.txt", map_index_filename)

        filtered_map_filename = os.path.join(tmp_dir, base_name + "_" + str(mapper_id) + "_filt_wline_no.map")
        shutil.move(base_name + "_filt_wline_no.map", filtered_map_filename)

        # Compress outputs
        zipped_map_index_filename = map_index_filename + ".bz2"
        with zipfile.ZipFile(zipped_map_index_filename, 'w', compression=zipfile.ZIP_BZIP2, compresslevel=9) as zf:
            zf.write(map_index_filename)

        zipped_filtered_map_filename = filtered_map_filename + ".bz2"
        with zipfile.ZipFile(zipped_filtered_map_filename, 'w', compression=zipfile.ZIP_BZIP2, compresslevel=9) as zf:
            zf.write(filtered_map_filename)

        # Copy intermediate files to storage for index correction
        map_index_key = os.path.join(pipeline_params.tmp_prefix, pipeline_params.execution_id, mapper_id,
                                     base_name + '_map.index.txt.bz2')
        storage.upload_file(file_name=zipped_map_index_filename, bucket=pipeline_params.storage_bucket,
                            key=map_index_key)

        filtered_map_key = os.path.join(pipeline_params.tmp_prefix, pipeline_params.execution_id, mapper_id,
                                        base_name + '_filt_wline_no.map.bz2')
        storage.upload_file(file_name=zipped_filtered_map_filename, bucket=pipeline_params.storage_bucket,
                            key=filtered_map_key)
    finally:
        os.chdir(pwd)
        force_delete_local_path(tmp_dir)

    return mapper_id, map_index_key, filtered_map_key


def index_correction(setname: str, bucket: str, exec_param: str, storage: Storage):
    """
    Corrects the index after the first map iteration.
    All the set files must have the prefix "map_index_files/".
    Corrected indices will be stored with the prefix "corrected_index/".

    Args:
        setname (str): files to be corrected
        bucket (str): s3 bucket where the set is stored
        exec_param (str): string used to differentiate this pipeline execution from others with different parameters
        storage (Storage): s3 storage instance, generated by lithops
    """

    # Download all files related to this set
    filelist = storage.list_keys(bucket, f'map_index_files/{exec_param}/{setname}')
    for file in filelist:
        local_file = file.split("/")[-1]
        storage.download_file(bucket, file, '/tmp/' + local_file)

    # Execute correction scripts
    cmd = f'/function/bin/binary_reducer.sh /function/bin/merge_gem_alignment_metrics.sh 4 /tmp/{setname}* > /tmp/{setname}.intermediate.txt'
    sp.run(cmd, shell=True, check=True, universal_newlines=True)
    cmd2 = f'/function/bin/filter_merged_index.sh /tmp/{setname}.intermediate.txt /tmp/{setname}'
    sp.run(cmd2, shell=True, check=True, universal_newlines=True)

    # Upload corrected index to storage
    storage.upload_file('/tmp/' + setname + '.txt', bucket, f'corrected_index/{exec_param}/{setname}.txt')


def filter_index_to_mpileup(self, old_id: int, fasta_chunk: dict, fastq_chunk: str, corrected_map_index_file: str,
                            filtered_map_file: str, base_name: str, exec_param: str, storage: Storage) -> Tuple[str]:
    """
    Second map  function, executed after the previous map function (map_alignment1) and the index correction.

    Args:
        old_id (int): id used in the previous map function
        fasta_chunk (dict): contains all the necessary info related to the fasta chunk
        fastq_chunk (str): fastq chunk key
        corrected_map_index_file (str): key of the corrected index stored in s3
        filtered_map_file (str): key of the filtered map file generated in the previous map function and stored in s3
        base_name (str): name used in the name of some of the generated files
        exec_param (str): string used to differentiate this pipeline execution from others with different parameters
        storage (Storage): s3 storage instance, generated by lithops

    Returns:
        Tuple[str]: keys to the generated txt and csv/parquet files (stored in s3)
    """

    ###################################################################
    #### RECOVER DATA FROM PREVIOUS MAP
    ###################################################################
    corrected_map_index_file = copy_to_runtime(storage, self.args.storage_bucket, f'corrected_index/{exec_param}/',
                                               corrected_map_index_file)
    filtered_map_file = copy_to_runtime(storage, self.args.storage_bucket, f'filtered_map_files/{exec_param}/',
                                        filtered_map_file)

    fasta_folder_file = fasta_chunk['key_fasta'].split("/")
    fasta = copy_to_runtime(storage, self.args.fasta_bucket, fasta_folder_file[0] + "/", fasta_folder_file[1],
                            {
                                'Range': f"bytes={fasta_chunk['chunk']['offset_base']}-{fasta_chunk['chunk']['last_byte+']}"},
                            fasta_chunk)  # Download fasta chunk

    with zipfile.ZipFile(filtered_map_file) as zf:
        zf.extractall("/")
    filtered_map_file = filtered_map_file.replace(".zip", "")

    ###################################################################
    #### FILTER ALIGNMENTS (CORRECT .map FILE)
    ###################################################################
    map_filtering_output = sp.run(
        ['/function/bin/map_file_index_correction.sh', corrected_map_index_file, filtered_map_file,
         str(self.args.tolerance)], capture_output=True)  # change to _v3.sh and runtime 20
    corrected_map_file = base_name + "_" + str(old_id) + "_filt_wline_no_corrected.map"

    ###################################################################
    #### GENERATE MPILEUP FROM MAP FILE
    ###################################################################
    mpileup_out = sp.run(['/function/bin/gempileup_run.sh', corrected_map_file, fasta], capture_output=True)
    mpileup_file = corrected_map_file + ".mpileup"

    ###################################################################
    #### CONVERT MPILEUP TO PARQUET / CSV
    ###################################################################
    format_key, text_key = self.mpileup_conversion(mpileup_file, fasta_chunk, fastq_chunk, exec_param, storage)

    return format_key, text_key

    ###################################################################
    ###################################################################
    #### AUXILIARY MAP FUNCTIONS
    ###################################################################
    ###################################################################


# def download_fastq(self, fastq_chunk: str) -> Tuple[str]:
#     """
#     Download fastq chunks depending on source
#
#     Args:
#         fastq_chunk (str): fastq chunk key
#
#     Returns:
#         Tuple[str]: name of the fastq file, if the sequence is single-end or paired-end and the name to be used in generated files
#     """
#
#     if self.args.seq_type == "paired-end":
#         fastq1 = fastq_to_mapfun(fastq_chunk[0], fastq_chunk[1])
#         base_name = os.path.splitext(fastq1)[0]  # +'.pe'
#         fastq2 = "yes"
#     else:  # single-end sequencing
#         fastq1 = fastq_to_mapfun(fastq_chunk[0], fastq_chunk[1])
#         fastq2 = "no"
#         base_name = os.path.splitext(fastq1)[0]  # +'.se'
#     return fastq1, fastq2, base_name


def mpileup_conversion(self, mpileup_file: str, fasta_chunk: dict, fastq_chunk: str, exec_param: str,
                       storage: Storage) -> Tuple[str]:
    """
    Convert resulting data to csv/parquet and txt

    Args:
        mpileup_file (str): mpileup file
        fasta_chunk (dict): contains all the necessary info related to the fasta chunk
        fastq_chunk (str): fastq chunk key
        exec_param (str): string used to differentiate this pipeline execution from others with different parameters
        storage (Storage): s3 storage instance, generated by lithops

    Returns:
        Tuple[str]: keys to the generated txt and csv/parquet files (stored in s3)
    """

    # Filter mpileup file
    with open(mpileup_file, 'r') as f:
        rows = f.read().splitlines()
        content = [row.split("\t") for row in rows]
        content.pop(-1)
        del rows

    # Convert mpileup to Pandas dataframe
    df = pd.DataFrame(data=content)
    df.columns = df.columns.astype(str)
    df['1'] = df['1'].astype(int64)

    # Remove disallowed characters
    fasta_key = self.fasta_chunks_prefix
    disallowed_characters = "._-!/·¡"
    for character in disallowed_characters:
        fasta_key = fasta_key.replace(character, "")

    # Create intermediate key
    fasta_chunk = str(fasta_chunk['id'])
    max_index = df.iloc[-1]['1']
    intermediate_key = self.args.file_format + "/" + exec_param + "/" + fasta_key + "_" + fasta_chunk + "-" + \
                       fastq_chunk[0] + "_chunk" + str(fastq_chunk[1]["number"]) + "_" + str(max_index)

    range_index = []
    x = 0
    while x < max_index:
        if (x < max_index):
            range_index.append(x)
        x = x + 100000

    x = x + (max_index - x)
    range_index.append(x)

    df3 = df.groupby(pd.cut(df['1'], range_index)).count()
    content = ""
    for i in range(len(df3)):
        content = content + str(range_index[i + 1]) + ":" + str(df3.iloc[i, 0]) + "\n"

    # Upload .txt file to storage
    storage.put_object(bucket=self.args.storage_bucket, key=intermediate_key + ".txt", body=content)

    # Write the mpileup file to the tmp directory
    if self.args.file_format == "csv":
        df.to_csv(mpileup_file + ".csv", index=False, header=False)
        with open(mpileup_file + ".csv", 'rb') as f:
            storage.put_object(bucket=self.args.storage_bucket, key=intermediate_key + ".csv", body=f)

    elif self.args.file_format == "parquet":
        df.to_parquet(mpileup_file + ".parquet")
        with open(mpileup_file + ".parquet", 'rb') as f:
            storage.put_object(bucket=self.args.storage_bucket, key=intermediate_key + ".parquet", body=f)

    return [intermediate_key + "." + self.args.file_format, intermediate_key + ".txt"]
