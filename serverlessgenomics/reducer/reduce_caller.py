import logging
from typing import Tuple
import lithops as deflithops

from ..parameters import PipelineRun, Lithops

from ..stats import Stats
from .reduce_functions import complete_multipart, create_multipart, create_multipart_keys, distribute_indexes, final_merge, finish, keys_by_fasta_split, reduce_function

logger = logging.getLogger(__name__)

def create_iterdata_reducer(intermediate_keys: dict, distributed_indexes: Tuple[Tuple[str]], multipart_ids: Tuple[str], 
                            multipart_keys: Tuple[str], pipeline_params: PipelineRun) -> Tuple[dict]:
    """
    Create the iterdata for the reduce stage.

    Args:
        intermediate_keys (Tuple[str]): Keys distributed by fasta split
        distributed_indexes (Tuple[Tuple[str]]): Indexes that each reducer should process
        multipart_ids (Tuple[str]): Multipart Upload IDs
        multipart_keys (Tuple[str]): Multipart Upload Keys
        pipeline_params (PipelineRun): Pipeline Parameters

    Returns:
        Tuple[dict]: Reduce stage iterdata
    """
    iterdata = []

    for keys, indexes, mpu_id, mpu_key in zip(intermediate_keys, distributed_indexes, multipart_ids, multipart_keys):
        start = 1
        n_part = 1

        for index in indexes:
            data = {
                "keys" : intermediate_keys[keys], 
                "range" : { "start" : start,
                            "end" : int(index)
                        },
                "mpu_id" : mpu_id,
                "n_part" : n_part,
                "mpu_key" : mpu_key,
                "pipeline_params": pipeline_params
            }
            iterdata.append(data)
            n_part += 1
            start = int(index) + 1

    return iterdata

def run_reducer(pipeline_params: PipelineRun, lithops, mapper_output):
    subStat = Stats()
    logger.debug("START OF REDUCE STAGE")
    
    #TODO: Fix errors with lithops cache and multipart upload ids
    lithops=deflithops.FunctionExecutor()
    

    # 1 Organize the keys generated by the map phase by fasta split
    intermediate_keys = keys_by_fasta_split(mapper_output)

    # 2 Create the keys for the multipart uploads
    multipart_keys = create_multipart_keys(pipeline_params)
 
    # 3 Create the multipart uploads and get their IDs
    multipart_ids = []
    for key in multipart_keys:
        multipart_ids.append(create_multipart(pipeline_params, key, lithops.storage))

    # 4 Select the indexes that each reducer will process
    indexes_iterdata = []
    for fasta in intermediate_keys:
        data = {
            'pipeline_params': pipeline_params,
            'keys': intermediate_keys[fasta]
        }
        indexes_iterdata.append(data)
    

    logger.debug("DISTRIBUTING INDEXES BETWEEN REDUCERS")
    subStat.timer_start('distribute_indexes')
    distributed_indexes = lithops.invoker.map(distribute_indexes, indexes_iterdata)
    subStat.timer_stop('distribute_indexes')
    distributed_indexes, timers = split_data_result(distributed_indexes)
    subStat.store_dictio(timers, "subprocesses", "distribute_indexes") 
    
    # 5 Launch the reducers
    logger.debug("EXECUTING REDUCE FUNCTION")
    reducer_iterdata = create_iterdata_reducer(intermediate_keys, distributed_indexes, multipart_ids, multipart_keys, pipeline_params)
    subStat.timer_start('reduce_function')
    reducer_output = lithops.invoker.map(reduce_function, reducer_iterdata)
    subStat.timer_stop('reduce_function')
    reducer_output, timers = split_data_result(reducer_output)
    subStat.store_dictio(timers, "subprocesses", "reduce_function")
    
    # 6 Complete the multipart uploads that the reducers created
    complete_multipart(multipart_keys, multipart_ids, reducer_output, pipeline_params, lithops.storage.storage_handler.s3_client)
    
    # 7 Create a multipart upload key and ID for the final file
    final_sinple_key = f'tmp/{pipeline_params.run_id}/final.alignment'
    final_id = create_multipart(pipeline_params, final_sinple_key, lithops.storage)
    
    # 8 Merge files created in stage 6 into one single file
    n_parts = len(multipart_keys)
    part = 1
    merge_iterdata = []
    while part <= n_parts:
        data = {
            "mpu_id": final_id,
            "mpu_key": final_sinple_key,
            "key": multipart_keys[part-1],
            "n_part": part,
            "pipeline_params": pipeline_params
        }
        merge_iterdata.append(data)
        part += 1
    

    logger.debug("EXECUTING FINAL MERGE")
    subStat.timer_start('final_merge')
    final_merge_results = lithops.invoker.map(final_merge, merge_iterdata)
    subStat.timer_stop('final_merge')
    final_merge_results, timers = split_data_result(final_merge_results)
    subStat.store_dictio(timers, "subprocesses", "final_merge")
       

    # 8 Complete the previous multipart upload
    finish(final_sinple_key, final_id, final_merge_results, pipeline_params, lithops.storage.storage_handler.s3_client)
    
    logger.debug("END OF REDUCE STAGE")
    return subStat

