"""

REMAINING THINGS TO DO:
-Implement new fasta partitioner
-Add s3 fastq location
-Improve/Remake log system
-Add the dockerfile folder
-More refactoring

"""

import argparse
import os.path
import time
import re
import sys
from random import randint
import Metadata
import fastq_functions as fq_func
import fasta_functions as fa_func

# map/reduce functions and executor
from map_reduce_executor import MapReduce
from map_functions import MapFunctions

CWD = os.getcwd()

###################################################################
#### PIPELINE SETTINGS
###################################################################
parser = argparse.ArgumentParser(description='Variant Caller - Cloudbutton Genomics Use Case demo')

###################################################################
#### COMMAND-LINE ARGUMENTS
###################################################################
# File Names And Locations
# fastq in SRA database
parser.add_argument('-fq','--fq_seq_name', help='Fastq sequence name (for example SRR6052133) used for SRA database',required=False)
# fastq in s3 bucket
parser.add_argument('-fq1','--fastq1', help='Fastq file 1, stored in s3',required=False)
parser.add_argument('-fq2','--fastq2', help='Fastq file 2, stored in s3 (paired end sequencing) - optional',required=False)
parser.add_argument('-fa','--fasta',help='Fasta reference filename', required=True)
# input files locations
parser.add_argument('-cl','--cloud_adr',help='cloud provider url prefix', required=False)
parser.add_argument('-b','--bucket',help='cloud provider bucket name', required=True)
parser.add_argument('-fb','--fbucket',help='cloud provider bucket name - for fasta file', required=False)
# fastq data source (SRA or s3)
parser.add_argument('-ds','--data_source',help='Data source', required=False)

# File Splitting Parameters
parser.add_argument('-nfq','--fastq_read_n', help='Number of reads per fastq chunk ',required=False)
parser.add_argument('-nfa','--fasta_char_n',help='Number of characters per fasta chunk', required=True)
parser.add_argument('-ofa','--fasta_char_overlap',help='bp overlap between fasta chunks', required=False)

# Pipeline-Specific Parameters
parser.add_argument('-t','--tolerance',help='number of additional strata to include in filtration of map file', required=False)
parser.add_argument('-ff','--file_format',help='mpileup file format - csv or parquet', required=False)

# Run Settings
parser.add_argument('-itn','--iterdata_n',help='Number of iterdata elements to run', required=False)
parser.add_argument('-cf','--concur_fun',help='concurrent function quota limit', required=False)
parser.add_argument('-s3w','--temp_to_s3',help='Write intermediate temp files to s3 for debugging', required=False)
parser.add_argument('-rt','--runtime_id',help='runtime to use to execute the map-reduce', required=False)
parser.add_argument('-rtm','--runtime_mem',help='runtime memory to be assigned to each function - maximum 2048 MB', required=False)
parser.add_argument('-rtr','--runtime_memr',help='runtime memory to be assigned to reduce function - maximum 10240 MB', required=False)
parser.add_argument('-rts','--runtime_storage',help='runtime storage to be assigned to map function - maximum 10000 MB - currently set manually', required=False)
parser.add_argument('-bs','--buffer_size',help='memory in percentatge for buffer size - maximum 100%', required=False)
parser.add_argument('-ftm','--func_timeout_map',help='timeout for map function - maximum 900', required=False)
parser.add_argument('-ftr','--func_timeout_reduce',help='timeout for reduce function - maximum 900', required=False)
parser.add_argument('-sk','--skip_map',help='True/False; use mpileups generated by previous run, to run only reducer', required=False)
parser.add_argument('-lb','--loadbalancer',help='load balancer execution method: manual|select', required=False)

###################################################################
#### PARSE COMMAND LINE ARGUMENTS
###################################################################
def parse_optional_arg(arg_opt1, arg_opt2, text2_opt1=None, text2_opt2=None):
    out1 = ""
    out2 = ""
    if arg_opt1 is not None:
        out1 = arg_opt1
        out2 = text2_opt1
    else:
        out1 = arg_opt2
        out2 = text2_opt2
    if out2 is not None:
        return(out1, out2)
    else:
        return(out1)
    
args = parser.parse_args()

# FastQ names
fq_seqname = parse_optional_arg(args.fq_seq_name, None)
fastq_file = parse_optional_arg(args.fastq1, "")

# From input, determine whether it is paired- or single-end sequencing
fastq_file2, seq_type = parse_optional_arg(args.fastq2, "","paired-end", "single-end")
fasta_file = args.fasta

# Cloud Storage Settings
cloud_adr = parse_optional_arg(args.cloud_adr, "aws")
BUCKET_NAME = args.bucket
FASTA_BUCKET = parse_optional_arg(args.fbucket, BUCKET_NAME)

# Fastq data source (SRA)
datasource = parse_optional_arg(args.data_source, "s3")

# File Splitting Parameters
# Fastq and fasta chunk sizes (fastq read no. multiplied by 4 to get number of lines)
fastq_read_n = int(parse_optional_arg(args.fastq_read_n, None))
fastq_chunk_size = 4*fastq_read_n  # used in the case of fastq stored in s3.
fasta_chunk_size = int(args.fasta_char_n)
fasta_char_overlap = int(parse_optional_arg(args.fasta_char_overlap, 300))

# Pipeline-Specific Parameters
tolerance = parse_optional_arg(args.tolerance, 0)
file_format = parse_optional_arg(args.file_format, "parquet")

# Run Settings
iterdata_n, function_n = parse_optional_arg(args.iterdata_n, None,args.iterdata_n, "all")
concur_fun = int(parse_optional_arg(args.concur_fun, 10000))
temp_to_s3 = parse_optional_arg(args.temp_to_s3, False)
runtime_id = parse_optional_arg(args.runtime_id, 'lumimar/hutton-genomics-v03:18')
runtime_mem = parse_optional_arg(args.runtime_mem, 1024)
runtime_mem_r = parse_optional_arg(args.runtime_memr, 4096)
runtime_storage = parse_optional_arg(args.runtime_storage, 4000)
buffer_size = parse_optional_arg(args.buffer_size, "75%")
func_timeout_map = parse_optional_arg(args.func_timeout_map, 2400)
func_timeout_reduce = parse_optional_arg(args.func_timeout_reduce, 2400)
skip_map = parse_optional_arg(args.skip_map, False)
lb_method = parse_optional_arg(args.loadbalancer, "select")

# DEBUGGING SETTINGS
gem_test=False              # To execute only the gem indexer and mapper and skip the rest of map function
pre_processing_only=False   # To skip map/reduce
debug=True                  # Keep all af.printl outputs, if not False don't print them

print("DEBUG_TEST: running only gem indexer and mapper in map function: "+str(gem_test))
print("DEBUG_TEST: running only pre-processing: "+str(pre_processing_only))

# S3 prefixes (cloud folders)
fasta_folder = "fasta/"
fastq_folder = "fastqgz/"
split_fasta_folder = "fasta-chunks/"
idx_folder = "fastq-indexes/"
out_folder = "outputs/"
s3_temp_folder = "temp_outputs/"


def generate_alignment_iterdata(list_fastq, list_fasta, iterdata_n):
    """
    Creates the lithops iterdata from the fasta and fastq chunk lists
    """
    os.chdir(CWD)
    iterdata = []
    
    # Number of fastq chunks processed. If doing a partial execution, iterdata_n will need to be multiple of the number of fasta chunks
    # so the index correction is done properly.
    num_chunks = 0  
    
    # Generate iterdata
    for fastq_key in list_fastq:
        num_chunks += 1
        for fasta_key in list_fasta:
            iterdata.append({'fasta_chunk': fasta_key, 'fastq_chunk': fastq_key})

    # Limit the length of iterdata if iterdata_n is not null.
    if iterdata_n is not None: 
        iterdata = iterdata[0:int(iterdata_n)]
        if(len(iterdata)%len(list_fasta)!=0):
            raise Exception("Hola")
        else: 
            num_chunks = len(iterdata)//len(list_fasta)

    return iterdata, num_chunks
 
 
if __name__ == "__main__":
    ###################################################################
    #### START THE PIPELINE
    ###################################################################
    run_id=str(randint(1000,9999))
    stage="PP"  
    id="X"      # this is for function id, which is not present in this case
    
    PP_start = time.time()

    # Run settings summary
    print("Variant Caller - Cloudbutton Genomics Use Case demo")
    print("starting pipeline at "+ str(time.time()) + " - " + str(time.strftime("%a, %d %b %Y %H:%M:%S +0000", time.gmtime())))
    
    print("run id: "+run_id)
    
    print("RUN SETTINGS")
    print("command line: ")
    print(sys.argv[0]+str(sys.argv))
    print("\nBucket name: %s\n" % BUCKET_NAME )
    print("Sequencing type: " + seq_type + "\n")

    print("INPUT FILES")
    print("Fastq file 1: %s" % fastq_file )
    print("Fastq file 2: %s" % fastq_file2 )
    print("Fasta file: %s" % fasta_file )

    print("\nFILE SPLITTING SETTINGS")
    print("Fastq chunk size: %s lines" % str(fastq_chunk_size) )
    print("Fasta chunk size: %s characters" % str(fasta_chunk_size) )
    print("Fasta overlap size: %s characters" % str(fasta_char_overlap) )

    print("\nOTHER RUN SETTINGS")
    if function_n == "all":
        print("Number of functions spawned: %s" % function_n )
    else:
        print("Number of functions spawned: %s" % iterdata_n )
    print("Runtime used: %s" % runtime_id )
    print("Runtime memory - map function: %s" % runtime_mem )
    print("Runtime memory - reduce function: %s" % runtime_mem_r )
    print("Runtime storage - map function: %s" % runtime_storage )
    print("Reduce load balancer method: %s" % lb_method )
    print("############\n")


    ###################################################################
    #### 1. GENERATE LIST OF FASTQ CHUNKS (BYTE RANGES)
    ###################################################################
    t0 = time.time()

    num_spots = 0
    metadata = Metadata.SraMetadata()
    arr_seqs = [fq_seqname]
    if datasource == "SRA":
        accession = metadata.efetch_sra_from_accessions(arr_seqs)
        seq_type = accession['pairing'].to_string(index=False)
        num_spots = accession['spots'].to_string(index=False)
        fastq_size = accession['run_size'].to_string(index=False)
        print("Retrieving data from sra...")
        print("Sequence type: " + seq_type)
        print("Number of spots: " + num_spots)
        print("fastq size: " + fastq_size)

    # Generate fastq chunks
    fastq_list = fq_func.prepare_fastq(fastq_read_n, fq_seqname, num_spots)

    t1 = time.time()
    print(f'PP:0: fastq list: execution_time: {t1 - t0}: s')


    ###################################################################
    #### 2. GENERATE LIST OF FASTA CHUNKS (if not present)
    ###################################################################
    t2 = time.time()

    # Fasta File Chunk Prefix:
    # the prefix contains the name of the fasta reference minus the suffix,
    # followed by block length and then "_split_" i.e. for hg19.fa
    # with chunks with block length of 400000 it would be hg19_400000_split_
    fasta_chunks_prefix = re.sub("\.fasta|\.fa|\.fas", "_" + str(fasta_chunk_size) + "split_", fasta_file)
    
    # Generate fasta chunks
    fasta_list = fa_func.prepare_fasta(cloud_adr, runtime_id, FASTA_BUCKET, fasta_folder, fasta_file, split_fasta_folder, fasta_chunk_size, fasta_chunks_prefix, fasta_char_overlap)
    
    t3 = time.time()
    print(f'PP:0: fasta list: execution_time: {t3 - t2}: s')

    
    ###################################################################
    #### 3. GENERATE ITERDATA
    ###################################################################
    # The iterdata consists of an array where each element is a pair of a fastq chunk and a fasta chunk.
    # Since each fastq chunk needs to be paired with each fasta chunk, the total number of elements in
    # iterdata will be n_fastq_chunks * n_fasta_chunks.
    
    print("\nGenerating iterdata")
    t4 = time.time()
    
    # Generate iterdata
    iterdata, num_chunks = generate_alignment_iterdata(fastq_list, fasta_list, iterdata_n)
    
    iterdata_n=len(iterdata)
    fastq_set_n=len(fasta_list) # number of fastq files aligned to each fasta chunk.
    
    
    ###################################################################
    #### 4. PREPROCESSING SUMMARY
    ###################################################################
    print("\nITERDATA LIST")
    print("number of fasta chunks: " + str(fastq_set_n))
    print("number of fastq chunks: " + str(len(fastq_list)))
    print("fasta x fastq chunks: "+ str(len(fasta_list)*len(fastq_list)))
    print("number of iterdata elements: " + str(iterdata_n))
    PP_end = time.time()
    print(f'PP:0: preprocessing: execution_time: {PP_end - PP_start}: s')


    if pre_processing_only==False:
        ###################################################################
        #### 5. MAP-REDUCE
        ###################################################################
        stage="MR"
        id=0
        
        mapfunc = MapFunctions(fq_seqname, datasource, seq_type, debug, BUCKET_NAME, fastq_folder, idx_folder, fasta_chunks_prefix, FASTA_BUCKET, stage, file_format, tolerance)

        mapreduce = MapReduce(mapfunc, runtime_id, runtime_mem, runtime_mem_r, buffer_size, file_format, func_timeout_map, func_timeout_reduce, 'DEBUG', BUCKET_NAME, stage, id, debug, skip_map, lb_method, iterdata_n, num_chunks, fq_seqname)
        map_time = mapreduce(iterdata)

        
        ###################################################################
        #### 6. MAP-REDUCE SUMMARY
        ###################################################################
        print("MAP-REDUCE SUMMARY")
        print("map phase: execution_time_total_varcall: "+str(map_time)+"s")